{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c4ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, magic, shutil\n",
    "from glob import glob\n",
    "import time, datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import joblib\n",
    "import datetime as dt\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, gc\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "#from skimage import io\n",
    "import sklearn\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss, f1_score, confusion_matrix, classification_report\n",
    "from sklearn import metrics, preprocessing\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "import wandb\n",
    "from catalyst.data.sampler import BalanceClassSampler\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d0796-1f1d-40df-ad4a-21f57ed7fe35",
   "metadata": {},
   "source": [
    "#### Hyper Param Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57c0283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'fold_num': 5,\n",
    "    'seed': 42,\n",
    "    't_model': 'tf_efficientnet_b2_ns',\n",
    "    'load_model': 'tf_efficientnet_b2_ns_20230422135321', # LOAD TEACHER MODEL\n",
    "    's_model': 'tf_efficientnet_lite2',\n",
    "    'img_size': 260,\n",
    "    'alpha': 0.5,\n",
    "    'epochs': 200,\n",
    "    'train_bs':128,\n",
    "    'valid_bs':64,\n",
    "    'lr': 1e-4, ## learning rate\n",
    "    'num_workers': 8,\n",
    "    'verbose_step': 1,\n",
    "    'patience' : 5,\n",
    "    'device': 'cuda:0',\n",
    "    'freezing': False,\n",
    "    'trainable_layer': 6,\n",
    "    'model_path': './models'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066a8a1-7060-4b6d-bf0c-d4164820a414",
   "metadata": {},
   "source": [
    "#### wandb init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "127461e4-c16d-47fd-b983-556c12ad0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = dt.datetime.now()\n",
    "run_id = time_now.strftime(\"%Y%m%d%H%M%S\")\n",
    "project_name = 'KD_efficientnet_ns_lite' + run_id\n",
    "user = 'hojunking'\n",
    "run_name = project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c538155-d55f-4321-bf7c-171d356ebceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 10Kwalk\n",
      "img_paths len : 1956\n",
      "\n",
      "label: battery\n",
      "img_paths len : 2805\n",
      "\n",
      "label: box\n",
      "img_paths len : 1988\n",
      "\n",
      "label: else\n",
      "img_paths len : 3785\n",
      "\n",
      "label: bottle\n",
      "img_paths len : 6152\n",
      "\n",
      "label: handkerchief\n",
      "img_paths len : 2433\n",
      "\n",
      "label: milk\n",
      "img_paths len : 2375\n",
      "\n",
      "label: paper\n",
      "img_paths len : 1665\n",
      "\n",
      "label: pet\n",
      "img_paths len : 2404\n",
      "\n",
      "label: plug\n",
      "img_paths len : 2931\n",
      "\n",
      "label: receipt\n",
      "img_paths len : 843\n",
      "\n",
      "label: shopping bag\n",
      "img_paths len : 1713\n",
      "\n",
      "label: stairs\n",
      "img_paths len : 4061\n",
      "\n",
      "label: transportation\n",
      "img_paths len : 2236\n",
      "\n",
      "label: trash picking\n",
      "img_paths len : 1628\n",
      "\n",
      "label: plate\n",
      "img_paths len : 5349\n",
      "\n",
      "Train_Images:  34962\n",
      "Train_Images_labels: 34962\n",
      "Test_Images:  9307\n",
      "Test_Images_labels: 9307\n"
     ]
    }
   ],
   "source": [
    "main_path = '../Data/carbon_data/'\n",
    "label_list = [\"10Kwalk\",\"battery\",'box','else','bottle','handkerchief',\n",
    "              'milk', 'paper', 'pet','plug','receipt', 'shopping bag', 'stairs',\n",
    "             'transportation', 'trash picking', 'plate']\n",
    "\n",
    "total_train_img_paths = []\n",
    "total_train_img_labels = []\n",
    "total_test_img_paths = []\n",
    "total_test_img_labels = []\n",
    "\n",
    "for label in label_list: ## 각 레이블 돌기\n",
    "    print(f'label: {label}')\n",
    "    img_paths = [] \n",
    "    img_labels = []\n",
    "\n",
    "    # default ratio\n",
    "    train_ratio = 1500\n",
    "    test_ratio = 500\n",
    "\n",
    "    dir_path = main_path + label ## 레이블 폴더 경로\n",
    "    count = 0\n",
    "    for folder, subfolders, filenames in os.walk(dir_path): ## 폴더 내 모든 파일 탐색\n",
    "    \n",
    "        for img in filenames: ## 각 파일 경로, 레이블 저장\n",
    "            count +=1\n",
    "            if count > train_ratio + test_ratio + 10000:\n",
    "                break\n",
    "            \n",
    "            img_paths.append(folder+'/'+img)\n",
    "            img_labels.append(label)\n",
    "        \n",
    "    random.shuffle(img_paths)\n",
    "    print(f'img_paths len : {len(img_paths)}\\n')\n",
    "\n",
    "    if label == 'else': ## 10walking 데이터 비율 설정하기 (데이터수: 2494)\n",
    "        train_ratio = 3000\n",
    "        test_ratio = 745\n",
    "    elif label == 'plate': \n",
    "        train_ratio = 4349\n",
    "        test_ratio = 1000\n",
    "    elif label == 'handkerchief':\n",
    "        train_ratio = 2000\n",
    "        test_ratio = 433     \n",
    "    elif label == 'milk':\n",
    "        train_ratio = 2000\n",
    "        test_ratio = 374\n",
    "    elif label == 'paper':\n",
    "        train_ratio = 1300\n",
    "        test_ratio = 365\n",
    "    elif label == 'pet':\n",
    "        train_ratio = 2000\n",
    "        test_ratio = 402\n",
    "    elif label == 'plug':\n",
    "        train_ratio = 2200\n",
    "        test_ratio = 725\n",
    "    elif label == 'receipt':\n",
    "        train_ratio = 600\n",
    "        test_ratio = 243 \n",
    "    elif label == 'shopping bag':\n",
    "        train_ratio = 1300\n",
    "        test_ratio = 413\n",
    "    elif label == 'stairs':\n",
    "        train_ratio = 3000\n",
    "        test_ratio = 1057\n",
    "    elif label == 'bottle':\n",
    "        train_ratio = 4652\n",
    "        test_ratio = 1500\n",
    "    elif label == 'transportation':\n",
    "        train_ratio = 1800\n",
    "        test_ratio = 435\n",
    "    elif label == 'trash picking':\n",
    "        train_ratio = 1300\n",
    "        test_ratio = 327\n",
    "    elif label == '10Kwalk':\n",
    "        train_ratio = 1556\n",
    "        test_ratio = 400\n",
    "    elif label == 'battery':\n",
    "        train_ratio = 2305\n",
    "        test_ratio = 500\n",
    "    elif label == 'box':\n",
    "        train_ratio = 1600\n",
    "        test_ratio = 388\n",
    "        \n",
    "    total_train_img_paths.extend(img_paths[:train_ratio])\n",
    "    total_train_img_labels.extend(img_labels[:train_ratio])\n",
    "\n",
    "    total_test_img_paths.extend(img_paths[-test_ratio:])\n",
    "    total_test_img_labels.extend(img_labels[-test_ratio:])\n",
    "\n",
    "print('Train_Images: ',len(total_train_img_paths))\n",
    "print(\"Train_Images_labels:\", len(total_train_img_labels))\n",
    "print('Test_Images: ',len(total_test_img_paths))\n",
    "print(\"Test_Images_labels:\", len(total_test_img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2716dd8b-84db-4707-80e2-19b29eae9c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10Kwalk_750.jpg</td>\n",
       "      <td>../Data/carbon_data/10Kwalk</td>\n",
       "      <td>10Kwalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10Kwalk_114.jpg</td>\n",
       "      <td>../Data/carbon_data/10Kwalk</td>\n",
       "      <td>10Kwalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10Kwalk_1450.jpg</td>\n",
       "      <td>../Data/carbon_data/10Kwalk</td>\n",
       "      <td>10Kwalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10Kwalk_634.jpg</td>\n",
       "      <td>../Data/carbon_data/10Kwalk</td>\n",
       "      <td>10Kwalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10Kwalk_94.jpg</td>\n",
       "      <td>../Data/carbon_data/10Kwalk</td>\n",
       "      <td>10Kwalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34957</th>\n",
       "      <td>leftover_1832.jpg</td>\n",
       "      <td>../Data/carbon_data/plate/leftover</td>\n",
       "      <td>plate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34958</th>\n",
       "      <td>wrap_371.jpg</td>\n",
       "      <td>../Data/carbon_data/plate/wrap</td>\n",
       "      <td>plate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34959</th>\n",
       "      <td>wrap_224.jpg</td>\n",
       "      <td>../Data/carbon_data/plate/wrap</td>\n",
       "      <td>plate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34960</th>\n",
       "      <td>wrap_166.jpg</td>\n",
       "      <td>../Data/carbon_data/plate/wrap</td>\n",
       "      <td>plate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34961</th>\n",
       "      <td>leftover_93.jpg</td>\n",
       "      <td>../Data/carbon_data/plate/leftover</td>\n",
       "      <td>plate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34962 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                image_id                                 dir    label\n",
       "0        10Kwalk_750.jpg         ../Data/carbon_data/10Kwalk  10Kwalk\n",
       "1        10Kwalk_114.jpg         ../Data/carbon_data/10Kwalk  10Kwalk\n",
       "2       10Kwalk_1450.jpg         ../Data/carbon_data/10Kwalk  10Kwalk\n",
       "3        10Kwalk_634.jpg         ../Data/carbon_data/10Kwalk  10Kwalk\n",
       "4         10Kwalk_94.jpg         ../Data/carbon_data/10Kwalk  10Kwalk\n",
       "...                  ...                                 ...      ...\n",
       "34957  leftover_1832.jpg  ../Data/carbon_data/plate/leftover    plate\n",
       "34958       wrap_371.jpg      ../Data/carbon_data/plate/wrap    plate\n",
       "34959       wrap_224.jpg      ../Data/carbon_data/plate/wrap    plate\n",
       "34960       wrap_166.jpg      ../Data/carbon_data/plate/wrap    plate\n",
       "34961    leftover_93.jpg  ../Data/carbon_data/plate/leftover    plate\n",
       "\n",
       "[34962 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pandas 데이터프레임 만들기\n",
    "trn_df = pd.DataFrame(total_train_img_paths, columns=['image_id'])\n",
    "trn_df['dir'] = trn_df['image_id'].apply(lambda x: os.path.dirname(x))\n",
    "trn_df['image_id'] = trn_df['image_id'].apply(lambda x: os.path.basename(x))\n",
    "trn_df['label'] = total_train_img_labels\n",
    "train = trn_df\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c67d39-7933-4f98-b998-54a6036ff4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "le = preprocessing.LabelEncoder()\n",
    "train['label'] = le.fit_transform(train['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3a8ac38-0b0f-4db4-80d2-6504916b7723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     4652\n",
       "9     4349\n",
       "4     3000\n",
       "13    3000\n",
       "1     2305\n",
       "10    2200\n",
       "5     2000\n",
       "6     2000\n",
       "8     2000\n",
       "14    1800\n",
       "3     1600\n",
       "0     1556\n",
       "7     1300\n",
       "12    1300\n",
       "15    1300\n",
       "11     600\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c454bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d97a3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path, sub_path=None):\n",
    "    try:\n",
    "        im_bgr = cv2.imread(path)\n",
    "        im_rgb = im_bgr[:, :, ::-1]\n",
    "        past_path = path\n",
    "    except: ## 이미지 에러 발생 시 백지로 대체\n",
    "        im_bgr = cv2.imread('../Data/carbon_reduction/temp_img.jpg')\n",
    "        im_rgb = im_bgr[:, :, ::-1]\n",
    "    return im_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "884eb987-4341-4fe7-8094-6e61cb051af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = A.Compose(\n",
    "    [\n",
    "        A.RandomResizedCrop(p=1, height=CFG['img_size'] ,width=CFG['img_size'], scale=(0.65, 0.75),ratio=(0.90, 1.10)),\n",
    "        A.SafeRotate(p=0.5, limit=(-20, 20), interpolation=2, border_mode=0, value=(0, 0, 0), mask_value=None),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ColorJitter(always_apply=True, p=0.5, contrast=0.2, saturation=0.3, hue=0.2),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "        A.pytorch.transforms.ToTensorV2()\n",
    "        ])\n",
    "\n",
    "transform_train_cap = A.Compose(\n",
    "    [\n",
    "        A.RandomResizedCrop(p=1, height=CFG['img_size'] ,width=CFG['img_size'], scale=(0.65, 0.85),ratio=(0.90, 1.10)),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "        A.pytorch.transforms.ToTensorV2()\n",
    "        ])\n",
    "\n",
    "transform_test = A.Compose(\n",
    "    [\n",
    "        A.Resize(height = CFG['img_size'], width = CFG['img_size']),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "        A.pytorch.transforms.ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f1561be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColonDataset(Dataset):\n",
    "    def __init__(self, df, data_root, transform=None, transform2=None, output_label=True):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transform = transform\n",
    "        self.transform2 = transform2\n",
    "        self.data_root = data_root\n",
    "        self.output_label = output_label\n",
    "        \n",
    "        if output_label == True:\n",
    "            self.labels = self.df['label'].values\n",
    "        \n",
    "        # EXEPTION TRANSFORM FOR CAPTURE IMAGES\n",
    "        self.cap_image = le.fit_transform(['10Kwalk', 'battery','receipt'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        # GET labels\n",
    "        if self.output_label:\n",
    "            target = self.labels[index]\n",
    "        # GET IMAGES\n",
    "        path = \"{}/{}\".format(self.data_root[index], self.df.iloc[index]['image_id'])\n",
    "        img  = get_img(path)\n",
    "        \n",
    "        # TRANSFORM1, TRANSFORM2 PROCESS\n",
    "        if self.transform:\n",
    "            if target in self.cap_image and self.transform2:\n",
    "                transformed = self.transform2(image=img)\n",
    "            else:\n",
    "                transformed = self.transform(image=img)\n",
    "            img = transformed['image']\n",
    "                \n",
    "        if self.output_label == True:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1601bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForcepImgClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class=2, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained, num_classes=n_class)\n",
    "        # n_features = self.model.classifier.in_features\n",
    "        # self.model.classifier = nn.Linear(n_features, n_class)\n",
    "    def freezing(self, freeze=False, trainable_layer = 2):\n",
    "        \n",
    "        if freeze:\n",
    "            num_layers = len(list(model.parameters()))\n",
    "            for i, param in enumerate(model.parameters()):\n",
    "                if i < num_layers - trainable_layer*2:\n",
    "                    param.requires_grad = False    \n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "240e5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(df, trn_idx, val_idx, data_root=train.dir.values):\n",
    "    \n",
    "    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n",
    "    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n",
    "    train_data_root = data_root[trn_idx]\n",
    "    valid_data_root = data_root[val_idx]\n",
    "    \n",
    "        \n",
    "    train_ds = ColonDataset(train_,\n",
    "                            train_data_root,\n",
    "                            transform=transform_train,\n",
    "                            transform2=transform_train_cap,\n",
    "                            output_label=True)\n",
    "    valid_ds = ColonDataset(valid_,\n",
    "                            valid_data_root,\n",
    "                            transform=transform_test,\n",
    "                            output_label=True)\n",
    "    # WEIGHTEDRANDOMSAMPLER\n",
    "    class_counts = train_.label.value_counts(sort=False).to_dict()\n",
    "    num_samples = sum(class_counts.values())\n",
    "    print(f'cls_cnts: {len(class_counts)}\\nnum_samples:{num_samples}')\n",
    "    \n",
    "    # weight 제작, 전체 학습 데이터 수를 해당 클래스의 데이터 수로 나누어 줌\n",
    "    class_weights = {l:round(num_samples/class_counts[l], 2) for l in class_counts.keys()}\n",
    "    t_labels = train_.label.to_list()\n",
    "    \n",
    "    # class 별 weight를 전체 trainset에 대응시켜 sampler에 넣어줌\n",
    "    weights = [class_weights[t_labels[i]] for i in range(int(num_samples))]\n",
    "\n",
    "\n",
    "    # weight 제작, 전체 학습 데이터 수를 해당 클래스의 데이터 수로 나누어 줌\n",
    "    class_weights = {l:round(num_samples/class_counts[l], 2) for l in class_counts.keys()}\n",
    "\n",
    "    # class 별 weight를 전체 trainset에 대응시켜 sampler에 넣어줌\n",
    "    weights = [class_weights[t_labels[i]] for i in range(int(num_samples))] \n",
    "    sampler = torch.utils.data.WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG['train_bs'],\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        sampler=sampler, \n",
    "        num_workers=CFG['num_workers']\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        valid_ds, \n",
    "        batch_size=CFG['valid_bs'],\n",
    "        num_workers=CFG['num_workers'],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67bc652d-dfff-45d6-8412-38db1fe187e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def distill_loss(student_logits, labels, teacher_logits, criterion, alpha=0.1):\n",
    "#     # TEACHER & STUDENT LOSS\n",
    "#     distillation_loss = criterion(student_logits, teacher_logits)\n",
    "    \n",
    "#     # STUDENT & LABEL LOSS\n",
    "#     student_loss = criterion(student_logits, labels)\n",
    "#     loss_b = alpha * student_loss + (1-alpha) * distillation_loss\n",
    "\n",
    "#     return loss_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0a7d551-5565-4c29-8398-371af4ece4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_loss(student_logits, labels, teacher_logits, criterion, alpha=0.1, temperature=2):\n",
    "    # STUDENT & LABEL LOSS\n",
    "    student_loss = criterion(student_logits, labels)\n",
    "\n",
    "    # TEACHER & STUDENT LOSS\n",
    "    teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n",
    "    student_probs = F.softmax(student_logits / temperature, dim=1)\n",
    "    distillation_loss = F.kl_div(torch.log(student_probs), teacher_probs, reduction=\"batchmean\") * (temperature ** 2)\n",
    "\n",
    "    # FINAL LOSS\n",
    "    loss_b = alpha * student_loss + (1 - alpha) * distillation_loss\n",
    "    return loss_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08ffa2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, s_model, t_model, loss_tr, optimizer, train_loader, device, scheduler=None, alpha =0.1):\n",
    "    t = time.time()\n",
    "\n",
    "    # SET MODEL TRAINING MODE\n",
    "    s_model.train()\n",
    "    t_model.eval()\n",
    "    \n",
    "    running_loss = None\n",
    "    loss_sum = 0\n",
    "    student_preds_all = []\n",
    "    image_targets_all = []\n",
    "    acc_list = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # STUDENT MODEL PREDICTION\n",
    "        with torch.cuda.amp.autocast():\n",
    "            student_preds = s_model(imgs)\n",
    "            \n",
    "            # TEACHER MODEL DISTILLATION (NO UPDATE)\n",
    "            with torch.no_grad():\n",
    "                teacher_preds = t_model(imgs)\n",
    "            \n",
    "            # DISTILLATION LOSS\n",
    "            loss = distill_loss(student_preds, image_labels, teacher_preds, loss_tr, alpha)\n",
    "            loss_sum+=loss.detach()\n",
    "            \n",
    "            # BACKPROPAGATION\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "            if running_loss is None:\n",
    "                running_loss = loss.item()\n",
    "            else:\n",
    "                running_loss = running_loss * .99 + loss.item() * .01    \n",
    "            \n",
    "            # TQDM VERBOSE_STEP TRACKING\n",
    "            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n",
    "                description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
    "                pbar.set_description(description)\n",
    "        \n",
    "        student_preds_all += [torch.argmax(student_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    student_preds_all = np.concatenate(student_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    \n",
    "    matrix = confusion_matrix(image_targets_all,student_preds_all)\n",
    "    epoch_f1 = f1_score(image_targets_all, student_preds_all, average='macro')\n",
    "    accuracy = (student_preds_all==image_targets_all).mean()\n",
    "    trn_loss = loss_sum/len(train_loader)\n",
    "    \n",
    "    return student_preds_all, accuracy, trn_loss, matrix, epoch_f1\n",
    "\n",
    "def valid_one_epoch(epoch,s_model, t_model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False, alpha =0.1):\n",
    "    ## Sets the model to valid mode\n",
    "    s_model.eval()\n",
    "    t_model.eval()\n",
    "\n",
    "    t = time.time()\n",
    "    loss_sum = 0\n",
    "    sample_num = 0\n",
    "    avg_loss = 0\n",
    "    student_preds_all = []\n",
    "    image_targets_all = []\n",
    "    \n",
    "    acc_list = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        student_preds = s_model(imgs)   #output = model(input)\n",
    "        teacher_preds = t_model(imgs) # teacher prediction\n",
    "        \n",
    "        loss = distill_loss(student_preds, image_labels, teacher_preds, loss_fn, alpha)\n",
    "        \n",
    "        student_preds_all += [torch.argmax(student_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        loss_sum += loss.item()*image_labels.shape[0]\n",
    "        sample_num += image_labels.shape[0]\n",
    "        \n",
    "        description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n",
    "        pbar.set_description(description)\n",
    "    \n",
    "    student_preds_all = np.concatenate(student_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    matrix = confusion_matrix(image_targets_all,student_preds_all)\n",
    "    \n",
    "    epoch_f1 = f1_score(image_targets_all, student_preds_all, average='macro')\n",
    "    acc = (student_preds_all==image_targets_all).mean()\n",
    "    val_loss = avg_loss/len(val_loader)\n",
    "    \n",
    "    return student_preds_all, acc, val_loss, matrix, epoch_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e4b92-23e3-4444-9369-1dc9dc504b13",
   "metadata": {},
   "source": [
    "#### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77e9950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            print(f'Best F1 score from now: {self.best_score}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37102a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhojunking\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hojun/git/carbon_reduction_project/wandb/run-20230422_221216-qfjormx5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hojunking/KD_efficientnet_ns_lite20230422220442/runs/qfjormx5\" target=\"_blank\">dutiful-dew-1</a></strong> to <a href=\"https://wandb.ai/hojunking/KD_efficientnet_ns_lite20230422220442\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: tf_efficientnet_lite2\n",
      "Training start with fold: 0 epoch: 200 \n",
      "\n",
      "cls_cnts: 16\n",
      "num_samples:27969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite2-dcccb7df.pth\" to /home/hojun/.cache/torch/hub/checkpoints/tf_efficientnet_lite2-dcccb7df.pth\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/tf_efficientnet_b2_ns_20230422135321/tf_efficientnet_b2_ns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# LOAD TEACHER_MODE WEIGHT\u001b[39;00m\n\u001b[1;32m     49\u001b[0m load_model \u001b[38;5;241m=\u001b[39m CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_model\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_model\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 50\u001b[0m teacher_model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# MODEL FREEZING\u001b[39;00m\n\u001b[1;32m     53\u001b[0m student_model\u001b[38;5;241m.\u001b[39mfreezing(freeze \u001b[38;5;241m=\u001b[39m CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfreezing\u001b[39m\u001b[38;5;124m'\u001b[39m], trainable_layer \u001b[38;5;241m=\u001b[39m CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainable_layer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/tf_efficientnet_b2_ns_20230422135321/tf_efficientnet_b2_ns'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    seed_everything(CFG['seed'])\n",
    "    \n",
    "    # WANDB TRACKER INIT\n",
    "    wandb.init(project=project_name, entity=user)\n",
    "    wandb.config.update(CFG)\n",
    "    wandb.run.name = run_name\n",
    "    wandb.define_metric(\"Train Accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train Loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train Macro F1 Score\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Macro F1 Score\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train-Valid Accuracy\", step_metric=\"epoch\")\n",
    "    \n",
    "    model_dir = CFG['model_path'] + '/{}_{}'.format(CFG['s_model'], run_id)\n",
    "    train_dir = train.dir.values\n",
    "    best_fold = 0\n",
    "    best_f1 =0.0\n",
    "    \n",
    "    print('Model: {}'.format(CFG['s_model']))\n",
    "    # MAKE MODEL DIR\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    \n",
    "    # STRATIFIED K-FOLD DEFINITION\n",
    "    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        print(f'Training start with fold: {fold} epoch: {CFG[\"epochs\"]} \\n')\n",
    "\n",
    "        # EARLY STOPPING DEFINITION\n",
    "        early_stopping = EarlyStopping(patience=CFG[\"patience\"], verbose=True)\n",
    "\n",
    "        # DATALOADER DEFINITION\n",
    "        train_loader, val_loader = prepare_dataloader(train, trn_idx, val_idx, data_root=train_dir)\n",
    "\n",
    "        # MODEL & DEVICE DEFINITION \n",
    "        device = torch.device(CFG['device'])\n",
    "        student_model = ForcepImgClassifier(CFG['s_model'], train.label.nunique(), pretrained=True)\n",
    "        teacher_model = ForcepImgClassifier(CFG['t_model'], train.label.nunique(), pretrained=True)\n",
    "\n",
    "        # T_MODEL DATA PARALLEL\n",
    "        teacher_model.to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            teacher_model = nn.DataParallel(teacher_model)\n",
    "\n",
    "        # LOAD TEACHER_MODE WEIGHT\n",
    "        load_model = CFG['model_path'] +'/' + CFG['load_model'] + '/' + CFG['t_model']\n",
    "        teacher_model.load_state_dict(torch.load(load_model))\n",
    "\n",
    "        # MODEL FREEZING\n",
    "        student_model.freezing(freeze = CFG['freezing'], trainable_layer = CFG['trainable_layer'])\n",
    "        if CFG['freezing'] ==True:\n",
    "            for name, param in student_model.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    print(f\"{name}: {param.requires_grad}\")\n",
    "\n",
    "        # S_MODEL DATA PARALLEL\n",
    "        student_model.to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            student_model = nn.DataParallel(student_model)\n",
    "\n",
    "        scaler = torch.cuda.amp.GradScaler()   \n",
    "        optimizer = torch.optim.Adam(student_model.parameters(), lr=CFG['lr'])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=5)\n",
    "\n",
    "        # DISTILLATION RATE\n",
    "        alpha = CFG['alpha']\n",
    "\n",
    "        # CRITERION (LOSS FUNCTION)\n",
    "        loss_tr = nn.CrossEntropyLoss().to(device) #MyCrossEntropyLoss().to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "        wandb.watch(student_model, loss_tr, log='all')\n",
    "\n",
    "        train_acc_list = []\n",
    "        train_matrix_list = []\n",
    "        train_f1_list = []\n",
    "        valid_acc_list = []\n",
    "        valid_matrix_list = []\n",
    "        valid_f1_list = []\n",
    "\n",
    "        start = time.time()\n",
    "        print(f'Fold: {fold}\\n')\n",
    "        for epoch in range(CFG['epochs']):\n",
    "            print('Epoch {}/{}'.format(epoch, CFG['epochs'] - 1))\n",
    "\n",
    "            # TRAINIG\n",
    "            train_preds_all, train_acc, train_loss, train_matrix, train_f1 = train_one_epoch(epoch, student_model, teacher_model,loss_tr, optimizer, train_loader, device, scheduler=scheduler, alpha = alpha)\n",
    "            wandb.log({'Train Accuracy':train_acc, 'Train Loss' : train_loss, 'Train F1': train_f1, 'epoch' : epoch})\n",
    "\n",
    "            # VALIDATION\n",
    "            with torch.no_grad():\n",
    "                valid_preds_all, valid_acc, valid_loss, valid_matrix, valid_f1= valid_one_epoch(epoch, student_model, teacher_model, loss_fn, val_loader, device, scheduler=None, alpha = alpha)\n",
    "                wandb.log({'Valid Accuracy':valid_acc, 'Valid Loss' : valid_loss, 'Valid F1': valid_f1 ,'epoch' : epoch})\n",
    "            print(f'Epoch [{epoch}], Train Loss : [{train_loss :.5f}] Val Loss : [{valid_loss :.5f}] Val F1 Score : [{valid_f1:.5f}]')\n",
    "\n",
    "            # SAVE ALL RESULTS\n",
    "            train_acc_list.append(train_acc)\n",
    "            train_matrix_list.append(train_matrix)\n",
    "            train_f1_list.append(train_f1)\n",
    "\n",
    "            valid_acc_list.append(valid_acc)\n",
    "            valid_matrix_list.append(valid_matrix)\n",
    "            valid_f1_list.append(valid_f1)\n",
    "\n",
    "            # MODEL SAVE (THE BEST MODEL OF ALL OF FOLD PROCESS)\n",
    "            if valid_f1 > best_f1:\n",
    "                best_f1 = valid_f1\n",
    "                torch.save(student_model.state_dict(), (model_dir+'/{}').format(CFG['s_model']))\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            stop = early_stopping(valid_f1)\n",
    "            if stop:\n",
    "                print(\"stop called\")   \n",
    "                break\n",
    "\n",
    "        end = time.time() - start\n",
    "        time_ = str(datetime.timedelta(seconds=end)).split(\".\")[0]\n",
    "        print(\"time :\", time_)\n",
    "\n",
    "        # PRINT BEST F1 SCORE MODEL OF FOLD\n",
    "        best_index = valid_f1_list.index(max(valid_f1_list))\n",
    "        print(f'fold: {fold}, Best Epoch : {best_index}/ {len(valid_f1_list)}')\n",
    "        print(f'Best Train Marco F1 : {train_f1_list[best_index]:.5f}')\n",
    "        print(train_matrix_list[best_index])\n",
    "        print(f'Best Valid Marco F1 : {valid_f1_list[best_index]:.5f}')\n",
    "        print(valid_matrix_list[best_index])\n",
    "        print('-----------------------------------------------------------------------')\n",
    "\n",
    "        ## K-FOLD END\n",
    "        if valid_f1_list[best_index] > best_fold:\n",
    "            best_fold = valid_f1_list[best_index]\n",
    "            top_fold = fold\n",
    "    print(f'Best Fold F1 score: {best_fold} Top fold : {top_fold}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084cd221-5c4f-4255-8f42-7e898c240898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
